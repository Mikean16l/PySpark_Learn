{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Get-ready-to-PySpark\" data-toc-modified-id=\"Get-ready-to-PySpark-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Get ready to PySpark</a></span></li><li><span><a href=\"#Loading-and-parsing-the-file\" data-toc-modified-id=\"Loading-and-parsing-the-file-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Loading and parsing the file</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-1\" data-toc-modified-id=\"Problem-1-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Problem 1</a></span></li><li><span><a href=\"#Problem-2\" data-toc-modified-id=\"Problem-2-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Problem 2</a></span></li></ul></li><li><span><a href=\"#Basic-counting-and-filtering\" data-toc-modified-id=\"Basic-counting-and-filtering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Basic counting and filtering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-3\" data-toc-modified-id=\"Problem-3-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Problem 3</a></span></li><li><span><a href=\"#Problem-4\" data-toc-modified-id=\"Problem-4-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Problem 4</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filtering-rows-with-'api_client'\" data-toc-modified-id=\"Filtering-rows-with-'api_client'-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Filtering rows with 'api_client'</a></span></li><li><span><a href=\"#Grouping-same-repos\" data-toc-modified-id=\"Grouping-same-repos-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Grouping same repos</a></span></li></ul></li></ul></li><li><span><a href=\"#Analytics\" data-toc-modified-id=\"Analytics-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Analytics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-5\" data-toc-modified-id=\"Problem-5-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Problem 5</a></span></li><li><span><a href=\"#Problem-6\" data-toc-modified-id=\"Problem-6-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Problem 6</a></span></li><li><span><a href=\"#Problem-7\" data-toc-modified-id=\"Problem-7-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Problem 7</a></span></li><li><span><a href=\"#Problem-8\" data-toc-modified-id=\"Problem-8-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Problem 8</a></span></li><li><span><a href=\"#Problem-9\" data-toc-modified-id=\"Problem-9-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Problem 9</a></span></li></ul></li><li><span><a href=\"#Indexing\" data-toc-modified-id=\"Indexing-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Indexing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-10\" data-toc-modified-id=\"Problem-10-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Problem 10</a></span></li><li><span><a href=\"#Problem-11\" data-toc-modified-id=\"Problem-11-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Problem 11</a></span></li><li><span><a href=\"#Problem-12\" data-toc-modified-id=\"Problem-12-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Problem 12</a></span></li><li><span><a href=\"#Problem-13\" data-toc-modified-id=\"Problem-13-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Problem 13</a></span></li></ul></li><li><span><a href=\"#Joining\" data-toc-modified-id=\"Joining-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Joining</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-14\" data-toc-modified-id=\"Problem-14-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Problem 14</a></span></li><li><span><a href=\"#Problem-15\" data-toc-modified-id=\"Problem-15-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Problem 15</a></span><ul class=\"toc-item\"><li><span><a href=\"#Method-1:-Join-by-'user_name/repo'\" data-toc-modified-id=\"Method-1:-Join-by-'user_name/repo'-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Method 1: Join by 'user_name/repo'</a></span></li><li><span><a href=\"#Method-2:-Join-by-'repo'\" data-toc-modified-id=\"Method-2:-Join-by-'repo'-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Method 2: Join by 'repo'</a></span></li></ul></li><li><span><a href=\"#Problem-16\" data-toc-modified-id=\"Problem-16-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Problem 16</a></span></li></ul></li><li><span><a href=\"#Dataframes\" data-toc-modified-id=\"Dataframes-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Dataframes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-17\" data-toc-modified-id=\"Problem-17-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Problem 17</a></span></li><li><span><a href=\"#Problem-18\" data-toc-modified-id=\"Problem-18-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Problem 18</a></span><ul class=\"toc-item\"><li><span><a href=\"#Count-rows-of-interesting-dataframe\" data-toc-modified-id=\"Count-rows-of-interesting-dataframe-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>Count rows of interesting dataframe</a></span></li><li><span><a href=\"#Join-on-repo-name\" data-toc-modified-id=\"Join-on-repo-name-8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;</span>Join on repo name</a></span></li><li><span><a href=\"#Most-failed-API-calls\" data-toc-modified-id=\"Most-failed-API-calls-8.2.3\"><span class=\"toc-item-num\">8.2.3&nbsp;&nbsp;</span>Most failed API calls</a></span></li></ul></li><li><span><a href=\"#Problem-19\" data-toc-modified-id=\"Problem-19-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Problem 19</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Spark\n",
    "\n",
    "                            By Mike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common uses of Spark is analyzing and processing log files. In this assignment, we will put Spark to good use for an OSS project that retrieves and downloads data from GitHub, called GHTorrent\n",
    "(http://ghtorrent.org). GHTorrent works by following the Github event timeline (https://api.github.com/events) and then retrieving all items linked from each event recursively and exhaustively. To make monitoring and debugging easier, the GHTorrent maintainers use extensive runtime logging for the downloader scripts.  \n",
    "\n",
    "Here is an extract of what the GHTorrent log looks like:  \n",
    "\n",
    "    DEBUG, 2017-03-23T10:02:27+00:00, ghtorrent-40 -- ghtorrent.rb: Repo EFForg/https-e verywhere exists  \n",
    "    DEBUG, 2017-03-24T12:06:23+00:00, ghtorrent-49 -- ghtorrent.rb: Repo Shikanime/print exists  \n",
    "    INFO, 2017-03-23T13:00:55+00:00, ghtorrent-42 -- api_client.rb: Successful request.\n",
    "    URL: https://api.github.com/repos/CanonicalLtd/maas-docs/issues/365/events?per_page=100, Remaining: 4943, Total: 88 ms\n",
    "    WARN, 2017-03-23T20:04:28+00:00, ghtorrent-13 -- api_client.rb: Failed request. URL: https://api.github.com/repos/greatfakeman/Tabchi/commits?sha=Tabchi&per_page=100, Status code: 404, Status: Not Found, Access: ac6168f8776, IP: 0.0.0.0, Remaining: 3031  \n",
    "    DEBUG, 2017-03-23T09:06:09+00:00, ghtorrent-2 -- ghtorrent.rb: Transaction committed (11 ms)  \n",
    "\n",
    "Each log line comprises of a standard part (up to .rb: ) and an operation-specific part. The standard part fields are like so:\n",
    "1. Logging level, one of DEBUG , INFO , WARN , ERROR (separated by , )\n",
    "2. A timestamp (separated by , )\n",
    "3. The downloader id, denoting the downloader instance (separated by -- )\n",
    "4. The retrieval stage, denoted by the Ruby class name, one of:\n",
    "      - event_processing\n",
    "      - ght_data_retrieval\n",
    "      - api_client\n",
    "      - retriever\n",
    "      - ghtorrent\n",
    "\n",
    "**Grade:** This assignment consists of 130 points. You need to collect 100 to get a 10!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ready to PySpark\n",
    "\n",
    "Import libs.  \n",
    "To access spark instance from jupyter notebook, we nned lib 'findspark'. Install it by simply `pip install` or `conda install -c conda-forge findspark`  \n",
    "If pyspark lib was install in the conda environmnet, one can skip this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-2.4.5-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If pyspark lib was install in the conda environmnet, one can skip this cell\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import sql, SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and parsing the file\n",
    "For the remaining of the assignment, you need to use this file\n",
    "(https://drive.google.com/file/d/0B9Rx0uhucsroYWJxdEpPd2JYcjg/view?usp=sharing) (~300MB compressed).\n",
    "Make sure you use the **correct kernel** in your notebook (either the PySpark kernel or the Apache Toree Scala kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "**T (5 points):** Download the log file and write a function to load it in an RDD. If you are doing this in Scala, make\n",
    "sure you use a case class to map the file fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build spark context methond 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two method to build spark context.\n",
    "scc = sql.SparkSession.builder.appName('Rdd1').config('spark.executer.memory','20g').getOrCreate()\n",
    "sc = scc.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build spark context methond 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Second Methond\n",
    "appName = 'Assignment_1'\n",
    "master = 'local'\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "# sc = sql.SparkSession(scc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if restart needed\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `appName` parameter is a name for your application to show on the cluster UI. `master` is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode. In practice, when running on a cluster, you will not want to hardcode `master` in the program, but rather launch the application with spark-submit and receive it there. However, for local testing and unit tests, you can pass “local” to run Spark in-process.  \n",
    "    https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "    https://spark.apache.org/docs/latest/submitting-applications.html#master-urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/ghtorrent-logs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(row):\n",
    "    row = row.replace(' ghtorrent-', ' ')\n",
    "    row = row.replace(' -- ', ', ')\n",
    "    row = row.replace('.rb: ', ', ')\n",
    "    return row.split(', ', 4)\n",
    "# split\n",
    "# https://www.runoob.com/python/att-string-split.html\n",
    "def readRDD(fileUrl):\n",
    "    log_file = sc.textFile(fileUrl)\n",
    "    return log_file.map(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "log_file = readRDD(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "**T (5 points):** How many lines does the RDD contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows of the log file is:  9669788\n"
     ]
    }
   ],
   "source": [
    "lines_rdd = log_file.count()\n",
    "print('Total rows of the log file is: ',lines_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic counting and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "**T (5 points):** Count the number of WARNing messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of WARNing messages is:  132158\n"
     ]
    }
   ],
   "source": [
    "# log_file.filter(lambda x: x[:4] == 'WARN').collect()\n",
    "numbers_WARN = log_file.filter(lambda x: x[0] == 'WARN').count()\n",
    "print('The number of WARNing messages is: ', numbers_WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "**T (10 points):** How many repositories where processed in total? Use the api_client lines only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return error, since some row doesn't conatin the 4th element.\n",
    "## log_file.filter(lambda x: x[3] == 'api_client').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see what are they (rows less than 4 elements) by runing this cell\n",
    "# log_file.filter(lambda y: len(y) < 4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_file.filter(lambda y: len(y) < 4).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering rows with 'api_client'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(row):\n",
    "    # this helper function will be apply to the filter\n",
    "    try:\n",
    "        return row[3] == 'api_client'\n",
    "    except IndexError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_api_client = log_file.filter(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping same repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = log_api_client.take(100)[0][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above variable gives something like:  \n",
    "    'Successful request. URL: https://api.github.com/repos/Particular/NServiceBus.Persistence.ServiceFabric/pulls/10/comments?per_page=100, Remaining: 3333, Total: 110 ms'  \n",
    "If we take a look at the URL, 'Particular' and 'NServiceBus.Persistence.ServiceFabric' indicate user name and repo name. So we can use these two info to group our data.  \n",
    "\n",
    "***The question asks to find out the number of processed repos which contains keyword 'api_client'. So the problem is what is a repository? For my understanding here, only if the URL includes 'repo', then it is a repository.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Particular', 'NServiceBus.Persistence.ServiceFabric']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to extract the following info inside the link\n",
    "a.split('/')[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRepo(row):\n",
    "    # This function is used for extracting the user name and repo name\n",
    "\n",
    "    if row[4].find('repos/') == -1:\n",
    "        row.append('')\n",
    "        return row\n",
    "    idx_after_repo = row[4].find('repos/') + 6\n",
    "    idx_first_slash_after_repo = row[4].find('/', idx_after_repo)\n",
    "    # Sometimes we cannot find another '/' after the repo. It is a '?'.\n",
    "    idx = row[4].find('/', idx_first_slash_after_repo + 1)\n",
    "    if idx == -1:\n",
    "        row.append(row[4][idx_after_repo: row[4].find('?')])\n",
    "    else:\n",
    "        row.append(row[4][idx_after_repo: idx])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we will apply the findRepo function to get all repos of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_api_repo = log_api_client.map(findRepo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out empty repo cols\n",
    "log_filter = log_api_repo.filter(lambda x: x[5] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884172"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by repo name\n",
    "log_group = log_filter.groupBy(lambda x: x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of api_client is:  71980\n"
     ]
    }
   ],
   "source": [
    "numbers_api_client = log_group.count()\n",
    "print('The number of api_client is: ', numbers_api_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "**T (5 points):** Which client did most HTTP requests?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnMax_byCol(rdd_data, col):\n",
    "    # define a function to return maximum count of the new added column.\n",
    "    # this function will be used many times for the following problems\n",
    "    rdd_data_group = rdd_data.groupBy(lambda x: x[col])\n",
    "    rdd_data_count = rdd_data_group.map(lambda x: (x[0], len(x[1])))\n",
    "    max_rdd_data = rdd_data_count.max(key = lambda x: x[1]) \n",
    "    return max_rdd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_id_count = returnMax_byCol(log_api_client,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The client number 13 did most HTTPS requst 135978.\n"
     ]
    }
   ],
   "source": [
    "print('The client number %s did most HTTPS requst %s.' \n",
    "      % (max_log_id_count[0], max_log_id_count[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "**T (5 points):** Which client did most FAILED HTTP requests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client_failed = log_api_client.filter(lambda x: x[4][:6] == 'Failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_client_failed = returnMax_byCol(api_client_failed, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The client number 13 did most failed HTTPS requst 79623.\n"
     ]
    }
   ],
   "source": [
    "print('The client number %s did most failed HTTPS requst %s.' \n",
    "      % (max_client_failed[0], max_client_failed[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n",
    "**T (5 points):** What is the most active hour of day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of empty rows\n",
    "log_filter = log_file.filter(lambda y: len(y) == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHour(line):\n",
    "    # this function will get the hours from datetime\n",
    "    idx_start = line[1].find('T') + 1\n",
    "    hour = line[1][idx_start : idx_start + 2]\n",
    "    line.append(hour)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_hourly = log_filter.map(getHour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another method to group by. \n",
    "# We can also use returnMax_byCol().\n",
    "log_mapping = log_hourly.map(lambda x: (x[5], x[:4]))\n",
    "# This works.\n",
    "log_hourly_group = log_mapping.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_hourly_count = log_hourly_group.map(lambda x: (x[0], len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_log_hourly = log_hourly_count.max(key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The the most active hour of day is at %s:00 o'clock, and there are %s requests\\ \n",
    "      within the hour.\" % (max_log_hourly[0], max_log_hourly[1]))\n",
    "# This will output the following:\n",
    "# The the most active hour of day is at 10:00 o'clock, \n",
    "# and there are 2662487 requests within the hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The the most active hour of day is at 10:00 o'clock, and there are 2662487 requests within the hour.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8\n",
    "**T (5 points):** What is the most active repository?  \n",
    "***Hint:*** use messages from the ghtorrent.rb layer only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This hint confused me a lot. In the previous problem 4, we have alrealy group the repos, and now we can just simply take the maximum of it. How to make use of ghtorrent.rb though??***\n",
    "\n",
    "To make use of this hint, I will do the following:  \n",
    "- Filter ghtorrent.rb\n",
    "- Get repo name in the related rows\n",
    "- Group by repo name\n",
    "- Find the maximum count by repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ghtorrent(row):\n",
    "    # this helper function will be apply to the filter\n",
    "    try:\n",
    "        return row[3] == 'ghtorrent'\n",
    "    except IndexError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ghtorrent = log_file.filter(find_ghtorrent)\n",
    "# log_ghtorrent.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then I find this plan is extremly hard! For example, take a look into these fromat:**  \n",
    "- 'Commit stevenndungu/Instat -> f9ec52148cc0f829766e0681456c67c36497c0dc exists'  \n",
    "- 'Repo node-inspector/node-inspector exists'\n",
    "- 'Fork creatobg/decoy is 0 commits ahead and 6 commits behind BKWLD/decoy'\n",
    "- 'Added project_language dovikos/mybatis-3 -> SQLPL (5509 bytes)'  \n",
    "    ...  \n",
    "    \n",
    "Repo names are in different locations, and followed by different indicators. For some rows, there are more than one repo name. So the following method will extract only one repo name from each rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repo_ghtorrent(row):\n",
    "    # define a helper function to get repo name in ghtorrent layer\n",
    "    try:\n",
    "        repo = row[4].split('/',1)\n",
    "        idx_space = repo[0][::-1].find(' ')\n",
    "        user_name = repo[0][::-1][:idx_space][::-1]\n",
    "        idx_space_repo = repo[1].find(' ')\n",
    "        repo_name = repo[1][:idx_space_repo]\n",
    "        repo= user_name + '/' + repo_name\n",
    "    except:\n",
    "        repo = ''\n",
    "    row.append(repo)\n",
    "    return row        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply helper and filter out rows without repo names.\n",
    "log_ghtorrent_repo = log_ghtorrent.map(repo_ghtorrent).filter(lambda x: x[5] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ghtorrent_repo = returnMax_byCol(log_ghtorrent_repo, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the most active repository is ovyx/HammerheadN, and there are 33799 records.\n"
     ]
    }
   ],
   "source": [
    "print(\"The the most active repository is %s, and there are %s records.\" \\\n",
    "                  % (max_ghtorrent_repo[0], max_ghtorrent_repo[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9\n",
    "**T (5 points):** Which access keys are failing most often?  \n",
    "***Hint:*** extract the `Access: ...` part from failing requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAccess(row):\n",
    "    # This function is used for extracting the access code\n",
    "    try:\n",
    "        idx_Access = row[4].find('Access:') + 8\n",
    "        idx_end_code = row[4].find(',', idx_Access)\n",
    "        access_code = row[4][idx_Access:idx_end_code]\n",
    "    except:\n",
    "        repo = ''\n",
    "    row.append(access_code)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of 'api_client_failed' of problem 6.\n",
    "log_access = api_client_failed.map(findAccess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_access = returnMax_byCol(log_access, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the access key: ac6168f8776 failed most and there are 79623 records.\n"
     ]
    }
   ],
   "source": [
    "print(\"The the access key: %s failed most and there are %s records.\" \\\n",
    "                  % (max_log_access[0], max_log_access[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "Typical operations on RDDs require grouping on a specific part of each record and then calculating specific counts given the groups. While this operation can be achieved with the group_by family of funcions, it is often useful to create a structure called an inverted index. An inverted index creates an 1..n mapping from the record part to all occurencies of the record in the dataset. For example, if the dataset looks like the following:\n",
    "\n",
    "    col1,col2,col3\n",
    "    A,1,foo\n",
    "    B,1,bar\n",
    "    C,2,foo\n",
    "    D,3,baz\n",
    "    E,1,foobar\n",
    "  \n",
    "  an inverted index on col2 would look like  \n",
    "  \n",
    "    1 -> [(A,1,foo), (B,1,bar), (E,1,foobar)]\n",
    "    2 -> [(C,2,foo)]\n",
    "    3 -> [(D,3,baz)]  \n",
    "  \n",
    "Inverted indexes enable us to quickly access precalculated partitions of the dataset. To see their effect on large datasets, lets compute an inverted index on the downloader id part.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10\n",
    "**T (10 points):** Create a function that given an RDD[Seq[T]] and an index position (denotes which field to index on), it computes an inverted index on the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(rdd, idx):\n",
    "    # Define a function to implement inverted index for rdd on idx.\n",
    "    inv_idx = rdd.groupBy(lambda x: x[idx])\n",
    "    return inv_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11\n",
    "**T (5 points):** Compute the number of different repositories accessed by the client ghtorrent-22 (without using the inverted index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of problem 4\n",
    "log_ghtorrent22 = log_filter.filter(lambda x: x[2] == '22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghtorrent22_repos = log_ghtorrent22.groupBy(lambda x: x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The client ghtorrent-22 has accessed 2743 different repos.\n"
     ]
    }
   ],
   "source": [
    "print('The client ghtorrent-22 has accessed %d different repos.'\\\n",
    "      % ghtorrent22_repos.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 12\n",
    "**T (5 points):** Compute the number of different repositories accessed by the client ghtorrent-22 (using the inverted index you calculated above). Remember that Spark computations are lazy, so you need to run the inverted index generation before you actually use the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ght_inv_idx = inverted_index(log_filter, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ght22_inv_idx = ght_inv_idx.filter(lambda x: x[0] == '22').collect()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.resultiterable.ResultIterable at 0x272aaaf8cc8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ght22_inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = []\n",
    "for row in ght22_inv_idx:\n",
    "    if row[5] not in lis:\n",
    "        lis.append(row[5])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The client ghtorrent-22 has accessed 2743 different repos.\n"
     ]
    }
   ],
   "source": [
    "print('The client ghtorrent-22 has accessed %d different repos.'\\\n",
    "      % len(lis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 13\n",
    "**T (5 points):** You should have noticed some difference in performance. Why is the indexed version faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Getting key value is faster, and it does do need to look up the same value in the data set, which costs more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining\n",
    "We now need to monitor the behaviour of interesting repositories. Use this link (https://drive.google.com/open?id=0B9Rx0uhucsroRHNVTFpzMV9OUGs) to download a list of repos into which we are interested to. This list was generated on Oct 10, 2017, more than 7 months after the log file was created. The format of the file is CSV,\n",
    "and the meaning of the fields can be found on the GHTorrent project web site documentation (http://ghtorrent.org/relational.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 14\n",
    "**T (5 points):** Read in the CSV file to an RDD (let's call it interesting). How many records are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = sc.textFile('data/important-repos.csv')\n",
    "interesting = csv_file.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1436 records in the csv file.\n"
     ]
    }
   ],
   "source": [
    "print('There are %d records in the csv file.' % interesting.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 15\n",
    "**T (10 points):** How many records in the log file refer to entries in the interesting file?  \n",
    "***Hint:*** Yes, you need to join :) First, you need to key both RDDs by the repository name to do such a join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Join by 'user_name/repo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCSVRepo(row):\n",
    "    # This function is used for extracting the user name and repo name\n",
    "    if row[1].find('repos/') == -1:\n",
    "        row.append('')\n",
    "        return row\n",
    "    idx_after_repo = row[1].find('repos/') + 6\n",
    "    idx_first_slash_after_repo = row[1].find('/', idx_after_repo)\n",
    "    row.append(row[1][idx_after_repo:])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_repo = interesting.map(findCSVRepo).filter(lambda x: x[9] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_repo_key = int_repo.map(lambda x: (x[9], x[:9]))\n",
    "log_filter_key = log_filter.map(lambda x: (x[5], x[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_repo = int_repo_key.join(log_filter_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using \"user_name/repo\" to join, there are 20410 records\n"
     ]
    }
   ],
   "source": [
    "print('By using \"user_name/repo\" to join, there are %d records' \\\n",
    "          % joined_repo.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Join by 'repo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeRepo(x):\n",
    "    x[5] = x[5].split(\"/\")[1]\n",
    "    return x\n",
    "\n",
    "interestingRepo = interesting.keyBy(lambda x: x[3])\n",
    "log_Repo = log_filter.map(changeRepo).keyBy(lambda x: x[5])\n",
    "joined_repo2 = interestingRepo.join(log_Repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using \"repo\" to join, there are 87962 records\n"
     ]
    }
   ],
   "source": [
    "print('By using \"repo\" to join, there are %d records' \\\n",
    "          % joined_repo2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The following will be using the second method to process.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 16\n",
    "**T (5 points):** Which of the interesting repositories has the most failed API calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_one(row):\n",
    "    # handling tuple\n",
    "    row = row[:1] + (1,)\n",
    "    return row\n",
    "\n",
    "# joined_repo has data structure like (a,([b,c])), this function will replace\n",
    "# ([b,c]) by 1.\n",
    "# https://thispointer.com/python-tuple-append-insert-modify-delete-elements-in-tuple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_count = joined_repo2.filter(lambda x: x[1][1][4][:6] == 'Failed')\\\n",
    "                           .map(replace_one)\\\n",
    "                           .reduceByKey(lambda x, y: x + y)\\\n",
    "                           .sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello-world', 740),\n",
       " ('test', 309),\n",
       " ('demo', 166),\n",
       " ('Test', 88),\n",
       " ('-', 47),\n",
       " ('hello', 26),\n",
       " ('Ruby_k59', 24),\n",
       " ('website', 20),\n",
       " ('TestRepo', 16),\n",
       " ('angular', 15)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_count.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repo hello-world has the most failed API calls, which is 740\n"
     ]
    }
   ],
   "source": [
    "print('The repo %s has the most failed API calls, which is %d' \\\n",
    "              % (failed_count.take(1)[0][0], failed_count.take(1)[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 17\n",
    "**T (10 points):** Read in the interesting repos file using Spark's CSV parser. Convert the log RDD to a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_df = sqlContext.read.format('csv').option('header','true') \\\n",
    "                        .option('inferSchema', 'true') \\\n",
    "                        .load('data/important-repos.csv');\n",
    "log_df = log_filter.map(changeRepo).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 18\n",
    "**T (15 points):** Repeat all 3 queries in the \"Joining\" section above using either SQL or the Dataframe API.\n",
    "Measure the time it takes to execute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count rows of interesting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has 1435 rows in total.\n",
      "Running lapsed time is: 0.07597661018371582\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print('CSV file has %d rows in total.' %interesting_df.count())\n",
    "print('Running elapsed time is:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join on repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Joined_data = interesting_df.join(log_df, log_df._6 \\\n",
    "                                  == interesting_df.name, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By using \"repo\" to join, there are 87930 records\n",
      "Running elapsed time is: 22.860700130462646\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print('By using \"repo\" to join, there are %d records' % Joined_data.count())\n",
    "print('Running elapsed time is:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most failed API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_count_df = Joined_data.filter(Joined_data._5.startswith('Failed'))\\\n",
    "                        .groupBy(Joined_data._6).count()\\\n",
    "                        .orderBy('count', ascending=False)\n",
    "\n",
    "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|         _6|count|\n",
      "+-----------+-----+\n",
      "|hello-world|  740|\n",
      "|       test|  309|\n",
      "|       demo|  166|\n",
      "|       Test|   88|\n",
      "|          -|   47|\n",
      "|      hello|   26|\n",
      "|   Ruby_k59|   24|\n",
      "|    website|   20|\n",
      "|   TestRepo|   16|\n",
      "|    angular|   15|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_count_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repo hello-world has the most failed API calls, which is 740\n",
      "Running elapsed time is: 53.735880613327026\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "count_max = join_count_df.take(1)[0].__getitem__('count')\n",
    "repo_max = join_count_df.take(1)[0].__getitem__('_6')\n",
    "print('The repo %s has the most failed API calls, which is %d' \\\n",
    "              % (repo_max, count_max))\n",
    "print('Running elapsed time is:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 19\n",
    "**T (5 points):** Select one of the queries and compare the execution plans between the RDD version and your version. (you can see them by going to localhost:4040 in your VM). What differences do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No ideas what's this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.gousios.gr/courses/bigdata/assignment-spark-solutions-python.html  \n",
    "- https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html  \n",
    "- https://stackoverflow.com/questions/34514545/spark-dataframe-groupby-and-sort-in-the-descending-order-pyspark  \n",
    "- https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html  \n",
    "- https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark_3.7",
   "language": "python",
   "name": "pyspark_3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
